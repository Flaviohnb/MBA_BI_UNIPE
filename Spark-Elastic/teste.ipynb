{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '927d8d22d8c7', 'cluster_name': 'docker-cluster', 'cluster_uuid': '038uq0CNScCWYvn2HwLEoA', 'version': {'number': '7.5.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '8bec50e1e0ad29dad5653712cf3bb580cd1afcdf', 'build_date': '2020-01-15T12:11:52.313576Z', 'build_snapshot': False, 'lucene_version': '8.3.0', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch('elasticsearch-node:9200')\n",
    "\n",
    "if es.indices.exists('stream-test'):\n",
    "    es.indices.delete('stream-test')\n",
    "    \n",
    "    body={\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'count': {'type': 'text'},\n",
    "                'name': {'type': 'text'},\n",
    "                'value': {'type': 'text'},\n",
    "                'timestamp': {'type': 'text'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    es.indices.create(index='stream-test', body=body)\n",
    "    \n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-7.5.2/dist/elasticsearch-spark-20_2.11-7.5.2.jar pyspark-shell'\n",
    "sc = SparkContext(appName=\"PythonSparkStreaming\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssc = StreamingContext(sc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n",
      "./sample/stream-sample0.22323477920407253.json\n",
      "{'count': 5, 'name': 'Bilbo', 'value': 37, 'timestamp': 1581297174}\n",
      "./sample/stream-sample0.0529028610560347.json\n",
      "{'count': 3, 'name': 'Bilbo', 'value': 66, 'timestamp': 1581297172}\n",
      "./sample/stream-sample0.5022360747241578.json\n",
      "{'count': 4, 'name': 'Legolas', 'value': 52, 'timestamp': 1581297173}\n",
      "./sample/stream-sample0.15689285269925923.json\n",
      "{'count': 11, 'name': 'Frodo', 'value': 4, 'timestamp': 1581297180}\n",
      "./sample/stream-sample0.8874662183716416.json\n",
      "{'count': 7, 'name': 'Legolas', 'value': 56, 'timestamp': 1581297176}\n",
      "./sample/stream-sample0.8519626057621469.json\n",
      "{'count': 8, 'name': 'Aragorn', 'value': 78, 'timestamp': 1581297177}\n",
      "./sample/stream-sample0.5509236364739196.json\n",
      "{'count': 0, 'name': 'Legolas', 'value': 54, 'timestamp': 1581297169}\n",
      "./sample/stream-sample0.47629301084795383.json\n",
      "{'count': 6, 'name': 'Samwise', 'value': 54, 'timestamp': 1581297175}\n",
      "./sample/stream-sample0.3436550314136364.json\n",
      "{'count': 9, 'name': 'Frodo', 'value': 91, 'timestamp': 1581297178}\n",
      "./sample/stream-sample0.43713427356771084.json\n",
      "{'count': 2, 'name': 'Bilbo', 'value': 79, 'timestamp': 1581297171}\n",
      "./sample/stream-sample0.7749946534185574.json\n",
      "{'count': 12, 'name': 'Legolas', 'value': 57, 'timestamp': 1581297181}\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "json_folder_path = ('./sample/')\n",
    "json_files = [ x for x in os.listdir(json_folder_path ) if x.endswith(\"json\") ]\n",
    "\n",
    "for json_file in json_files:\n",
    "    json_file_path = os.path.join(json_folder_path, json_file)\n",
    "    with open (json_file_path) as f:\n",
    "        print(json_file_path)\n",
    "        data_dict = json.loads(f.read())\n",
    "        print(data_dict)\n",
    "        \n",
    "#stream = ssc.textFileStream(json_file_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_write_conf = {\n",
    "    \"es.nodes\" : '927d8d22d8c7',\n",
    "    \"es.port\" : '9200',\n",
    "    \"es.resource\" : 'stream-test',\n",
    "    \"es.input.json\" : \"yes\",\n",
    "    \"es.mapping.id\": \"count\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(x):\n",
    "    return (data_dict['count'], json.dumps(data_dict))\n",
    "\n",
    "rdd = rdd.map(lambda x: format_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsNewAPIHadoopFile(\n",
    "    path='-',\n",
    "    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_write_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_read_conf = {\n",
    "    \"es.nodes\" : '927d8d22d8c7',\n",
    "    \"es.port\" : '9200',\n",
    "    \"es.resource\" : 'stream-test'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12',\n",
       "  {'count': '12',\n",
       "   'name': 'Legolas',\n",
       "   'value': '57',\n",
       "   'timestamp': '1581297181'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
