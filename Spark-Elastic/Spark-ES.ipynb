{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Elasticsearch Integration\n",
    "\n",
    "<b>Este bloco de notas demonstra como você pode usar o Spark para ler dados de streaming, realizar uma transformação nos dados e gravar os novos dados no Elasticsearch.\n",
    "\n",
    "Também demonstra como você pode usar o Spark para ler um índice do Elasticsearch.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando o ES-Hadoop\n",
    "\n",
    "<b>Primeiro, precisamos instalar o conector Elastic-hadoop. \n",
    "Isso deve ser instalado no mesmo caminho em todo o cluster.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtendo a configuração do Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.listdir('../../file/'))\n",
    "#os.chdir('../../file/')\n",
    "\n",
    "#!pip install elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Primeiro configure a conexão do Elasticsearch por padrão, nos conectamos ao elasticsearch: 9200, como estamos executando este notebook no Spark-Node, precisamos usar 'elasticsearch' em vez de 'localhost', pois esse é o nome do docker ontainer executando o Elasticsearch. Se o índice de teste de fluxo existir, limpe-o e crie um novo. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'aacde1e0f861', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'd1A5rIAySvWisXB_BvTmXA', 'version': {'number': '7.5.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '8bec50e1e0ad29dad5653712cf3bb580cd1afcdf', 'build_date': '2020-01-15T12:11:52.313576Z', 'build_snapshot': False, 'lucene_version': '8.3.0', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch('elasticsearch-node:9200')\n",
    "#es = Elasticsearch('http://localhost:9200/')\n",
    "\n",
    "#es.indices.create('stream-test')\n",
    "\n",
    "if es.indices.exists('stream-test'):\n",
    "    es.indices.delete('stream-test')\n",
    "    \n",
    "    body={\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'count': {'type': 'text'},\n",
    "                'name': {'type': 'text'},\n",
    "                'value': {'type': 'text'},\n",
    "                'timestamp': {'type': 'text'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    es.indices.create(index='stream-test', body=body)\n",
    "    #es.indices.put_mapping(index='stream-test', doc_type='sample')\n",
    "    #es.index(index='stream-test', doc_type='sample', body=body)\n",
    "        \n",
    "\n",
    "    \n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming para Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Precisamos garantir que o conector ES-Hadoop esteja no caminho de classe do driver. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "import os  \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-7.5.2/dist/elasticsearch-spark-20_2.11-7.5.2.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Agora podemos criar nosso Contexto Spark (é o ponto de entrada para qualquer funcionalidade do spark). </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonSparkStreaming\")  \n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>...e nosso contexto de streaming ( representa a conexão com um cluster Spark e pode ser usado para criar várias fontes de entrada). </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Em seguida, vamos gerar um fluxo de arquivos. Nesse caso, transmitiremos todos os arquivos gravados no diretório de amostra. Isso está sendo bombeado com dados aleatórios.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ssc.textFileStream('sample/')\n",
    "#print(os.listdir(os.getcwd()+'/sample/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vamos querer realizar algumas transformações leves nesses dados. Primeiramente, queremos analisar o horário da época em uma sequência de datas que o Elasticsearch possa entender.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(x):\n",
    "    data = json.loads(x)\n",
    "    data['timestamp'] = datetime.fromtimestamp(data['timestamp']).strftime('%Y/%m/%d %H:%M:%S')\n",
    "    data['doc_id'] = data.pop('count')\n",
    "    return (data['doc_id'], json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Mapearemos a função que acabamos de criar para o fluxo, para que cada registro, em cada lote, seja analisado com a mesma função.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = stream.map(lambda x: format_sample(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Em seguida, vamos definir uma função que grave o RDD gerado por cada operação em lote de streaming no Elasticsearch. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(rdd):\n",
    "        es_write_conf = {\n",
    "        # especifique o nó para o qual estamos enviando dados (esse deve ser o mestre)\n",
    "        \"es.nodes\" : 'aacde1e0f861',\n",
    "            \n",
    "        # especifique a porta caso ela não seja a porta padrão\n",
    "        \"es.port\" : '9200',\n",
    "            \n",
    "        # especifique um recurso no formato '<index>/<type>'\n",
    "        \"es.resource\" : 'stream-test',\n",
    "\n",
    "        # e a entrada JSON?\n",
    "        \"es.input.json\" : \"yes\",\n",
    "            \n",
    "        # O campo do documento / nome da propriedade que contém o ID do documento.\n",
    "        \"es.mapping.id\": \"doc_id\",\n",
    "        }\n",
    "\n",
    "        rdd.saveAsNewAPIHadoopFile(\n",
    "                path='/',\n",
    "                outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "                keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "                valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "\n",
    "                # criticamente, devemos especificar nosso 'es_write_conf' \n",
    "                conf=es_write_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Agora podemos aplicar nosso manipulador a cada registro transmitido pelo sistema.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.foreachRDD(lambda rdd: handler(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Podemos solicitar que também seja impresso em 'stdout'.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Finalmente, podemos começar o contexto do spark.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Se queremos parar o contexto, precisamos chamar o método 'stop'</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento em massa ES com Spark (Bulk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vamos gerar um novo contexto do spark desde que matamos nosso último. Usaremos isso para operar em um índice inteiro de dados de ES usando o Spark. Nesse caso, leremos novamente os dados que acabamos de enviar usando o Spark Streaming.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonSparkReading\")  \n",
    "sc.setLogLevel(\"WARN\")\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_read_conf = { \n",
    "    # especifique o nó para o qual estamos enviando dados (esse deve ser o mestre)    \n",
    "    \"es.nodes\" : \"aacde1e0f861\",\n",
    "    \n",
    "    # especifique o recurso de leitura no formato '<index>/<type>'\n",
    "    \"es.resource\" : \"stream-test\"\n",
    "    }\n",
    "\n",
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vamos pegar uma amostra dos dados que acabamos de extrair do Elasticsearch. Observe que cada objeto é uma tupla, onde o primeiro item é o ID do documento.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "es_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vamos converter essas tuplas em JSON puro</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd = es_rdd.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Agora, vamos converter o RDD em um Dataframe Spark SQL para que possamos tratá-lo mais como um objeto Pandas</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "# Executa um 'monkey patch' no contexto do Spark, estendendo-o com recursos do Spark SQL\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = es_rdd.map(lambda l: Row(**dict(l))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Podemos executar um 'groupby' nos dados. Nesse caso, agruparemos por nome </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .groupby('name') \\\n",
    "    .count() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Também podemos filtrar dados como no 'Pandas'.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .filter(df.name == 'Samwise')\\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Novamente, precisamos parar o contexto, se quisermos voltar para um Contexto de Streaming</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
